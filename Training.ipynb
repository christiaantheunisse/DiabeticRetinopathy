{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "359f9261",
   "metadata": {},
   "source": [
    "# Get started"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1913536",
   "metadata": {},
   "source": [
    "For now:\n",
    "\n",
    "- ~~Load the data~~\n",
    "- ~~Import the ViT~~\n",
    "- ~~Add the classification layers~~\n",
    "- ~~Setup the training loop~~\n",
    "- ~~Divide code over separate files~~\n",
    "- ~~Validation set~~\n",
    "- ~~Calculate accuracy during training on validation set~~\n",
    "\n",
    "Future task (separate notebooks):\n",
    "- ~~Data inspection~~ -> Created a smaller dataset to train faster\n",
    "- Data augmentation -> Can be simply done in dataset class\n",
    "\n",
    "Questions / things to look into:\n",
    "- what are the class tokens -> _An image is worth 16 x 16 words_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2079ac9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "from types import MethodType\n",
    "\n",
    "from functools import partial\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "import timm\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from typing import Callable\n",
    "from torchsummary import summary\n",
    "\n",
    "from heads import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0509cda8-38b0-4b1c-a3e2-c942ea3a4e1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# In case you gpu memory remains occupied by PyTorch after restarting the kernel\n",
    "torch.cuda.empty_cache() "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6aa3718",
   "metadata": {},
   "source": [
    "## Set the configuration\n",
    "The finetuned model is trained for classification on imageNet, so the MLP head on the ViT is also trained. However, since the MLP head is removed from the vision transformer to make it an encoder, this shouldn't be a problem. The encoder weights are just different (if they were not frozen during finetuning. I am not sure, but I don't think that's the case). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1fd7d1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_SIZE = \"base\"  # options are ['base', 'large', 'huge']\n",
    "WEIGHTS_VERSION = \"pretrained\"  # options are ['pretrained', 'finetuned']\n",
    "WEIGHTS_FOLDER = \"weights\"\n",
    "NO_CLASSES = 5\n",
    "\n",
    "DATA_FOLDER = \"data\"\n",
    "TRAIN_FOLDER = \"train\"\n",
    "TRAIN_LABELS_CSV = \"reducedTrainLabels.csv\"  # \"trainLabels.csv\"\n",
    "\n",
    "BATCH_SIZE = 4\n",
    "EPOCHS = 90\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "335f03ea",
   "metadata": {},
   "source": [
    "# Import the pre-trained MAE ViT\n",
    "\n",
    "This [Github repository](https://github.com/facebookresearch/mae) provides a PyTorch implementation of the paper [Masked Autoencoders Are Scalable Vision Learners](https://arxiv.org/abs/2111.06377). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3db29a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: implement freeze of whole model except head\n",
    "def prepare_vision_transformer(\n",
    "    checkpoint_directory: str,\n",
    "    model_architecture: dict,\n",
    "    classification_head: nn.Module,\n",
    "):\n",
    "    \"\"\"\n",
    "    This function returns the vision transformer with the right head and weights.\n",
    "    Arguments:\n",
    "        checkpoint_directory (string): directory where the weights of the ViT are stored\n",
    "        model_architecture (Callable): function that instantiates the ViT with certain settings\n",
    "        classification_head (nn.Module): The classification head that will be attached directly to the ViT\n",
    "    \"\"\"\n",
    "    vision_transformer = timm.models.vision_transformer.VisionTransformer(**model_architecture)\n",
    "    # To ensure that the weights of the head are not set by the pretrained weights\n",
    "    vision_transformer.head = None\n",
    "\n",
    "    checkpoint = torch.load(checkpoint_directory)\n",
    "\n",
    "    msg = vision_transformer.load_state_dict(checkpoint[\"model\"], strict=False)\n",
    "    print(msg)\n",
    "\n",
    "    vision_transformer.head = classification_head\n",
    "\n",
    "    return vision_transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ef39c93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Architectures according to the original ViT paper: An image is worth 16x16 words\n",
    "BASE_VIT = {\n",
    "    \"patch_size\": 16,\n",
    "    \"embed_dim\": 768,\n",
    "    \"depth\": 12,\n",
    "    \"num_heads\": 12,\n",
    "    \"mlp_ratio\": 4,\n",
    "    \"qkv_bias\": True,\n",
    "    \"norm_layer\": partial(nn.LayerNorm, eps=1e-6),\n",
    "}\n",
    "LARGE_VIT = {\n",
    "    \"patch_size\": 16,\n",
    "    \"embed_dim\": 1024,\n",
    "    \"depth\": 24,\n",
    "    \"num_heads\": 16,\n",
    "    \"mlp_ratio\": 4,\n",
    "    \"qkv_bias\": True,\n",
    "    \"norm_layer\": partial(nn.LayerNorm, eps=1e-6),\n",
    "}\n",
    "HUGE_VIT = {\n",
    "    \"patch_size\": 14,\n",
    "    \"embed_dim\": 1280,\n",
    "    \"depth\": 32,\n",
    "    \"num_heads\": 16,\n",
    "    \"mlp_ratio\": 4,\n",
    "    \"qkv_bias\": True,\n",
    "    \"norm_layer\": partial(nn.LayerNorm, eps=1e-6),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "719b58ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose the weights and the architecture\n",
    "chkpts_finetuned = {\n",
    "    \"base\": \"mae_finetuned_vit_base.pth\",\n",
    "    \"large\": \"mae_finetuned_vit_large.pth\",\n",
    "    \"huge\": \"mae_finetuned_vit_huge.pth\",\n",
    "}\n",
    "chkpts_pretrained = {\n",
    "    \"base\": \"mae_pretrain_vit_base.pth\",\n",
    "    \"large\": \"mae_pretrain_vit_large.pth\",\n",
    "    \"huge\": \"mae_pretrain_vit_huge.pth\",\n",
    "}\n",
    "chkpts = {'pretrained': chkpts_pretrained, 'finetuned': chkpts_finetuned}[WEIGHTS_VERSION]\n",
    "\n",
    "model_architectures= {\n",
    "    \"base\": BASE_VIT,\n",
    "    \"large\": LARGE_VIT,\n",
    "    \"huge\": HUGE_VIT,\n",
    "}\n",
    "\n",
    "model_arch = model_architectures[MODEL_SIZE]\n",
    "chkpt_dir = os.path.join(WEIGHTS_FOLDER, chkpts[MODEL_SIZE])\n",
    "print(f\"Weights directory: \\n\\t{chkpt_dir}\\nModel architecture: \\n\\t{model_arch}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37e8b25c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The heads are defined in heads.py\n",
    "ViT_HEAD = OneLayer(model_arch['embed_dim'], NO_CLASSES)\n",
    "# ViT_HEAD = PassThrough()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a72a522",
   "metadata": {},
   "outputs": [],
   "source": [
    "# instantiate the model\n",
    "vision_transformer = prepare_vision_transformer(\n",
    "    checkpoint_directory=chkpt_dir,\n",
    "    model_architecture=model_arch,\n",
    "    classification_head=ViT_HEAD,\n",
    ")\n",
    "# Output should be: <All keys matched successfully>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "052b12cf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "summary(vision_transformer, (3, 224, 224), device='cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86bfd792",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Load the data\n",
    "https://pytorch.org/tutorials/beginner/data_loading_tutorial.html\n",
    "\n",
    "A class that contains the data. Extra data augmentation can be easily added. I already implemented the resize since the input images do not have the same size, which causes error when making a torch.Tensor with a batch of images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "629f60b0-9665-407e-ad3c-82e8b789d96d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from data import DiabeticRetinopathyDataset, Resize\n",
    "from torchvision import transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import data\n",
    "from importlib import reload  # Python 3.4+\n",
    "reload(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9de832cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "DR_dataset = data.DiabeticRetinopathyDataset(\n",
    "    TRAIN_LABELS_CSV,\n",
    "    DATA_FOLDER,\n",
    "    TRAIN_FOLDER,\n",
    "    transform=transforms.Compose([Resize(output_size=224)]),  # output size depends on the model\n",
    "    size=40,\n",
    ")\n",
    "# train_set, val_set = DR_dataset.train_val_split(split_rate=0.8)\n",
    "generator = torch.Generator().manual_seed(42)\n",
    "train_set, val_set = torch.utils.data.random_split(DR_dataset, [0.8, 0.2], generator=generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61b73b3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize some data\n",
    "def visualise_batch(images, labels):\n",
    "    for i, im in enumerate(images):\n",
    "        ax = plt.subplot(1, len(labels), i+1)\n",
    "        ax.set_title(f\"{labels[i].tolist()}\")\n",
    "        ax.imshow(im.permute(1, 2, 0))\n",
    "    \n",
    "visualise_batch(*train_set[[1, 2, 3]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60b237fd-2476-4e71-9997-983e858eb6fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_count = np.unique(DR_dataset.labels, return_counts=True)\n",
    "print(\" label | count \\n\" + \\\n",
    "      \"-------|-------\")\n",
    "display = lambda c : str(c) + \" \" * (6-len(str(c)))\n",
    "for label, count in zip(*label_count):\n",
    "    print(f\"   {label}   | {display(count)}\") "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f080236a",
   "metadata": {},
   "source": [
    "# Training loop\n",
    "https://pytorch.org/tutorials/beginner/introyt/trainingyt.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3ad206e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import clear_output\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from training import train_one_epoch, validate, save_model\n",
    "from WeightedKappaLoss import WeightedKappaLoss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db6bd957",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_loader = torch.utils.data.DataLoader(train_set, batch_size=BATCH_SIZE, shuffle=True)\n",
    "validation_loader = torch.utils.data.DataLoader(val_set, batch_size=BATCH_SIZE, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e773f598",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = WeightedKappaLoss(num_classes=5, mode='quadratic')\n",
    "acc_fn = WeightedKappaLoss(num_classes=5, mode='quadratic', validate=True)  # Returns a slightly different value\n",
    "\n",
    "# From 'Masked Autoencoders Are Scalable Vision Learners' their linear probing procedure\n",
    "blr = 0.1\n",
    "lr = blr * BATCH_SIZE / 256\n",
    "optimizer = torch.optim.SGD(vision_transformer.parameters(), lr=lr, momentum=0.9)\n",
    "lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a4f6d1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Freeze the whole model, except the classification head\n",
    "for param in vision_transformer.parameters():\n",
    "    param.requires_grad = False\n",
    "for param in vision_transformer.head.parameters():\n",
    "    param.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5253118c-54ed-4979-8fcd-77d5ea02082d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the name of the directory you want to save the current training session in.\n",
    "# If the directory does not exist, it will be automatically created.\n",
    "RUN_NAME = \"Try_out\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "711ed0af",
   "metadata": {},
   "outputs": [],
   "source": [
    "vision_transformer.to(DEVICE)\n",
    "train_losses, val_losses, val_accs = [], [], []\n",
    "for i in range(EPOCHS):\n",
    "    epoch = i + 1\n",
    "    vision_transformer.train(True)\n",
    "    \n",
    "    # Train over all training data\n",
    "    avg_train_loss = train_one_epoch(model=vision_transformer,\n",
    "                               epoch_index=epoch,\n",
    "                               training_loader=training_loader,\n",
    "                               optimizer=optimizer,\n",
    "                               loss_fn=loss_fn,\n",
    "                              )\n",
    "    train_losses.append(avg_train_loss)\n",
    "    \n",
    "    # Set the model to validation mode\n",
    "    vision_transformer.eval()\n",
    "    \n",
    "    # Validation on validation data\n",
    "    avg_val_loss, avg_val_acc = validate(model=vision_transformer,\n",
    "                                         epoch_index=epoch,\n",
    "                                         validation_loader=validation_loader,\n",
    "                                         loss_fn=loss_fn,\n",
    "                                         acc_fn=acc_fn\n",
    "                                        )\n",
    "    val_losses.append(avg_val_loss)\n",
    "    val_accs.append(avg_val_acc)    \n",
    "    \n",
    "    # Save model\n",
    "    save_model(vision_transformer, epoch, RUN_NAME)\n",
    "    \n",
    "    # plot statistics\n",
    "    clear_output(wait=True)\n",
    "    epochs_range = np.arange(i+1)\n",
    "    ax = plt.subplot(1,2,1)\n",
    "    ax.plot(epochs_range, np.array(train_losses), label=\"Train loss\")\n",
    "    ax.plot(epochs_range, np.array(val_losses), label=\"Val loss\")\n",
    "    ax.legend()\n",
    "    ax = plt.subplot(1,2,2)\n",
    "    ax.plot(epochs_range, np.array(val_accs), label=\"Val acc\")\n",
    "    ax.legend()\n",
    "    plt.show()\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39c02c9e-1be6-476d-a7c0-d2d6206985ed",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71761665-1963-42e8-bcff-e403589e43ad",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
