{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5cf9da9f",
   "metadata": {},
   "source": [
    "# Run the models inside a directory on the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4978bde",
   "metadata": {},
   "outputs": [],
   "source": [
    "from data import DiabeticRetinopathyDataset, CropBlack, Resize\n",
    "from torchvision import transforms\n",
    "import torch\n",
    "import timm\n",
    "import os\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from functools import partial\n",
    "from torchsummary import summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b649e39",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_ROOT_DIR = \"models\"\n",
    "MODEL_NAME = \"Kaggle One Layer\"\n",
    "\n",
    "DATA_FOLDER = \"data\"\n",
    "TEST_FOLDER = \"test\"\n",
    "TEST_LABELS_CSV = \"testLabels.csv\"\n",
    "\n",
    "WEIGHTS_VERSION = \"pretrained\"  # Don't change\n",
    "WEIGHTS_FOLDER = \"weights\"\n",
    "\n",
    "NO_CLASSES = 5\n",
    "BATCH_SIZE = 10\n",
    "\n",
    "MODEL_SIZE = \"large\"  # options are ['base', 'large', 'huge']\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c45434f",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "472ea6bb",
   "metadata": {},
   "source": [
    "## Load the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d541102f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: implement freeze of whole model except head\n",
    "def prepare_vision_transformer(\n",
    "    checkpoint_directory: str,\n",
    "    model_architecture: dict,\n",
    "    classification_head: nn.Module,\n",
    "):\n",
    "    \"\"\"\n",
    "    This function returns the vision transformer with the right head and weights.\n",
    "    Arguments:\n",
    "        checkpoint_directory (string): directory where the weights of the ViT are stored\n",
    "        model_architecture (Callable): function that instantiates the ViT with certain settings\n",
    "        classification_head (nn.Module): The classification head that will be attached directly to the ViT\n",
    "    \"\"\"\n",
    "    vision_transformer = timm.models.vision_transformer.VisionTransformer(**model_architecture)\n",
    "    # To ensure that the weights of the head are not set by the pretrained weights\n",
    "    vision_transformer.head = None\n",
    "\n",
    "    checkpoint = torch.load(checkpoint_directory)\n",
    "\n",
    "    msg = vision_transformer.load_state_dict(checkpoint[\"model\"], strict=False)\n",
    "    print(msg)\n",
    "\n",
    "    vision_transformer.head = classification_head\n",
    "\n",
    "    return vision_transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f14f45c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Architectures according to the original ViT paper: An image is worth 16x16 words\n",
    "BASE_VIT = {\n",
    "    \"patch_size\": 16,\n",
    "    \"embed_dim\": 768,\n",
    "    \"depth\": 12,\n",
    "    \"num_heads\": 12,\n",
    "    \"mlp_ratio\": 4,\n",
    "    \"qkv_bias\": True,\n",
    "    \"norm_layer\": partial(nn.LayerNorm, eps=1e-6),\n",
    "}\n",
    "LARGE_VIT = {\n",
    "    \"patch_size\": 16,\n",
    "    \"embed_dim\": 1024,\n",
    "    \"depth\": 24,\n",
    "    \"num_heads\": 16,\n",
    "    \"mlp_ratio\": 4,\n",
    "    \"qkv_bias\": True,\n",
    "    \"norm_layer\": partial(nn.LayerNorm, eps=1e-6),\n",
    "}\n",
    "HUGE_VIT = {\n",
    "    \"patch_size\": 14,\n",
    "    \"embed_dim\": 1280,\n",
    "    \"depth\": 32,\n",
    "    \"num_heads\": 16,\n",
    "    \"mlp_ratio\": 4,\n",
    "    \"qkv_bias\": True,\n",
    "    \"norm_layer\": partial(nn.LayerNorm, eps=1e-6),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44e1335c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose the weights and the architecture\n",
    "chkpts_finetuned = {\n",
    "    \"base\": \"mae_finetuned_vit_base.pth\",\n",
    "    \"large\": \"mae_finetuned_vit_large.pth\",\n",
    "    \"huge\": \"mae_finetuned_vit_huge.pth\",\n",
    "}\n",
    "chkpts_pretrained = {\n",
    "    \"base\": \"mae_pretrain_vit_base.pth\",\n",
    "    \"large\": \"mae_pretrain_vit_large.pth\",\n",
    "    \"huge\": \"mae_pretrain_vit_huge.pth\",\n",
    "}\n",
    "chkpts = {'pretrained': chkpts_pretrained, 'finetuned': chkpts_finetuned}[WEIGHTS_VERSION]\n",
    "\n",
    "model_architectures= {\n",
    "    \"base\": BASE_VIT,\n",
    "    \"large\": LARGE_VIT,\n",
    "    \"huge\": HUGE_VIT,\n",
    "}\n",
    "\n",
    "model_arch = model_architectures[MODEL_SIZE]\n",
    "chkpt_dir = os.path.join(WEIGHTS_FOLDER, chkpts[MODEL_SIZE])\n",
    "print(f\"Weights directory: \\n\\t{chkpt_dir}\\nModel architecture: \\n\\t{model_arch}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae33be17",
   "metadata": {},
   "outputs": [],
   "source": [
    "from heads import OneLayer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92deeaa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The heads are defined in heads.py\n",
    "ViT_HEAD = OneLayer(model_arch['embed_dim'], NO_CLASSES)\n",
    "# ViT_HEAD = PassThrough()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa7d24f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# instantiate the model\n",
    "vision_transformer = prepare_vision_transformer(\n",
    "    checkpoint_directory=chkpt_dir,\n",
    "    model_architecture=model_arch,\n",
    "    classification_head=ViT_HEAD,\n",
    ")\n",
    "# Output should be: <All keys matched successfully>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c763586",
   "metadata": {},
   "outputs": [],
   "source": [
    "summary(vision_transformer, (3, 224, 224), device='cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "954bb0be",
   "metadata": {},
   "source": [
    "## Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce269e21",
   "metadata": {},
   "outputs": [],
   "source": [
    "DR_test_set = DiabeticRetinopathyDataset(\n",
    "    TEST_LABELS_CSV,\n",
    "    DATA_FOLDER,\n",
    "    TEST_FOLDER,\n",
    "    transform=transforms.Compose([CropBlack(),\n",
    "                                  Resize(output_size=224)]),\n",
    "    sample_rates=None,\n",
    "#     size=50,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "174705f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(DR_test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "701de826",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize some data\n",
    "def visualise_batch(images, labels):\n",
    "    for i, im in enumerate(images):\n",
    "        ax = plt.subplot(1, len(labels), i+1)\n",
    "        ax.set_title(f\"{labels[i].tolist()}\")\n",
    "        ax.imshow(im.permute(1, 2, 0))\n",
    "    \n",
    "visualise_batch(*DR_test_set[[1, 2, 3]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30aecd4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_count = np.unique(DR_test_set.labels, return_counts=True)\n",
    "print(\" label | count \\n\" + \\\n",
    "      \"-------|-------\")\n",
    "display = lambda c : str(c) + \" \" * (6-len(str(c)))\n",
    "for label, count in zip(*label_count):\n",
    "    print(f\"   {label}   | {display(count)}\") "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7277c4bd",
   "metadata": {},
   "source": [
    "## Import the head weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e974133",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "weight_files = np.array([f for f in os.listdir(os.path.join(MODEL_ROOT_DIR, MODEL_NAME))\n",
    "                        if os.path.isfile(os.path.join(MODEL_ROOT_DIR, MODEL_NAME, f))])\n",
    "\n",
    "# 'OneLayer_35.pth' is for example shown before 'OneLayer_4.pth', so fix this\n",
    "numbers = []\n",
    "for idx, f in enumerate(weight_files):\n",
    "    name = f.split(\"_\")[0]\n",
    "    number = (f.split(\".\")[0]).split(\"_\")[1]\n",
    "    number = \"0\" + number if len(number) == 1 else number\n",
    "    numbers.append(number)\n",
    "\n",
    "# Start with the latest epoch\n",
    "weight_files = weight_files[np.argsort(numbers)][::-1]  \n",
    "weight_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e3db25a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_head_weights(model: nn.Module, weight_file: str):\n",
    "    checkpoint_dir = os.path.join(MODEL_ROOT_DIR, MODEL_NAME, weight_file)\n",
    "    checkpoint = torch.load(checkpoint_dir)\n",
    "    \n",
    "    msg = model.head.load_state_dict(checkpoint, strict=True)\n",
    "    print(msg)\n",
    "    \n",
    "    return model\n",
    "\n",
    "vision_transformer = load_head_weights(vision_transformer, weight_files[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18254554",
   "metadata": {},
   "source": [
    "## Run the testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a39d1d0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import clear_output\n",
    "from tqdm import tqdm\n",
    "from WeightedKappaLoss import WeightedKappaLoss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b59d2757",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loader = torch.utils.data.DataLoader(DR_test_set, batch_size=BATCH_SIZE, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "434e40e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_fn = WeightedKappaLoss(num_classes=5, mode='quadratic', validate=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a240570c",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "vision_transformer.to(DEVICE)\n",
    "test_accs = []\n",
    "\n",
    "### Run the testing for one weight file\n",
    "# --> Pick the epoch the weight file belongs to: \n",
    "epoch = 31\n",
    "weight_file = weight_files[-epoch] \n",
    "print(f\"Analyzing weights from {weight_file}\")\n",
    "# Load the weights\n",
    "checkpoint_dir = os.path.join(MODEL_ROOT_DIR, weight_file)\n",
    "checkpoint = torch.load(checkpoint_dir)\n",
    "\n",
    "msg = vision_transformer.head.load_state_dict(checkpoint, strict=True)\n",
    "print(msg)\n",
    "\n",
    "# Put the model in evaluation mode\n",
    "vision_transformer.eval()\n",
    "_, avg_test_acc = validate(model=vision_transformer,\n",
    "                           epoch_index=epoch,\n",
    "                           validation_loader=test_loader,\n",
    "                           loss_fn=None,\n",
    "                           acc_fn=acc_fn,\n",
    "                          )\n",
    "test_accs.append(avg_test_acc)\n",
    "\n",
    "print(f\"The Quadratic Weighted Kappa Score on the test set for {weight_file} is: {avg_test_acc}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fda395b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
