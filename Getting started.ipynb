{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "359f9261",
   "metadata": {},
   "source": [
    "# Get started"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1913536",
   "metadata": {},
   "source": [
    "For now:\n",
    "- ~~Load the data ~~\n",
    "- ~~Import the ViT~~\n",
    "- ~~Add the classification layers~~\n",
    "- ~~Setup the training loop~~\n",
    "- Divide code over separate files\n",
    "- Validation set\n",
    "- Calculate accuracy during training on validation set\n",
    "\n",
    "Future task (separate notebooks):\n",
    "- Data inspection\n",
    "- Data augmentation -> Can be simply done in dataset class\n",
    "\n",
    "Questions / things to look into:\n",
    "- what are the class tokens -> _An image is worth 16 x 16 words_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2079ac9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "from types import MethodType\n",
    "\n",
    "from functools import partial\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "import timm\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from typing import Callable\n",
    "from torchsummary import summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6aa3718",
   "metadata": {},
   "source": [
    "## Set the configuration\n",
    "The finetuned model is trained for classification on imageNet, so the MLP head on the ViT is also trained. However, since the MLP head is removed from the vision transformer to make it an encoder, this shouldn't be a problem. The encoder weights are just different (if they were not frozen during finetuning. I am not sure, but I don't think that's the case). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1fd7d1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_SIZE = \"base\"  # options are ['base', 'large', 'huge']\n",
    "WEIGHTS_VERSION = \"pretrained\"  # options are ['pretrained', 'finetuned']\n",
    "WEIGHTS_FOLDER = \"weights\"\n",
    "NO_CLASSES = 5\n",
    "EMBED_DIM = 768  # hardcoded right now, but depends on ViT architecture\n",
    "\n",
    "DATA_FOLDER = \"data\"\n",
    "TRAIN_FOLDER = \"train\"\n",
    "TRAIN_LABELS_CSV = \"trainLabels.csv\"\n",
    "\n",
    "BATCH_SIZE = 4\n",
    "EPOCHS = 2\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "335f03ea",
   "metadata": {},
   "source": [
    "# Import the pre-trained MAE ViT\n",
    "\n",
    "This [Github repository](https://github.com/facebookresearch/mae) provides a PyTorch implementation of the paper [Masked Autoencoders Are Scalable Vision Learners](https://arxiv.org/abs/2111.06377). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3db29a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: implement freeze of whole model except head\n",
    "def prepare_vision_transformer(\n",
    "    checkpoint_directory: str,\n",
    "    model_architecture: dict,\n",
    "    classification_head: nn.Module,\n",
    "):\n",
    "    \"\"\"\n",
    "    This function returns the vision transformer with the right head and weights.\n",
    "    Arguments:\n",
    "        checkpoint_directory (string): directory where the weights of the ViT are stored\n",
    "        model_architecture (Callable): function that instantiates the ViT with certain settings\n",
    "        classification_head (nn.Module): The classification head that will be attached directly to the ViT\n",
    "    \"\"\"\n",
    "    vision_transformer = timm.models.vision_transformer.VisionTransformer(**model_architecture)\n",
    "    # To ensure that the weights of the head are not set by the pretrained weights\n",
    "    vision_transformer.head = None\n",
    "\n",
    "    checkpoint = torch.load(checkpoint_directory)\n",
    "\n",
    "    msg = vision_transformer.load_state_dict(checkpoint[\"model\"], strict=False)\n",
    "    print(msg)\n",
    "\n",
    "    vision_transformer.head = classification_head\n",
    "\n",
    "    return vision_transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ef39c93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Architectures according to the original ViT paper: An image is worth 16x16 words\n",
    "BASE_VIT = {\n",
    "    \"patch_size\": 16,\n",
    "    \"embed_dim\": 768,\n",
    "    \"depth\": 12,\n",
    "    \"num_heads\": 12,\n",
    "    \"mlp_ratio\": 4,\n",
    "    \"qkv_bias\": True,\n",
    "    \"norm_layer\": partial(nn.LayerNorm, eps=1e-6),\n",
    "}\n",
    "LARGE_VIT = {\n",
    "    \"patch_size\": 16,\n",
    "    \"embed_dim\": 1024,\n",
    "    \"depth\": 24,\n",
    "    \"num_heads\": 16,\n",
    "    \"mlp_ratio\": 4,\n",
    "    \"qkv_bias\": True,\n",
    "    \"norm_layer\": partial(nn.LayerNorm, eps=1e-6),\n",
    "}\n",
    "HUGE_VIT = {\n",
    "    \"patch_size\": 14,\n",
    "    \"embed_dim\": 1280,\n",
    "    \"depth\": 32,\n",
    "    \"num_heads\": 16,\n",
    "    \"mlp_ratio\": 4,\n",
    "    \"qkv_bias\": True,\n",
    "    \"norm_layer\": partial(nn.LayerNorm, eps=1e-6),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82879126",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Architectures taken from https://github.com/facebookresearch/mae/blob/efb2a8062c206524e35e47d04501ed4f544c0ae8/models_vit.py#L56-L74\n",
    "# def vit_base_patch16(**kwargs):\n",
    "#     \"\"\"ViT-Base as defined in: An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale\"\"\"\n",
    "#     model = timm.models.vision_transformer.VisionTransformer(\n",
    "#         patch_size=16, embed_dim=768, depth=12, num_heads=12, mlp_ratio=4, qkv_bias=True,\n",
    "#         norm_layer=partial(nn.LayerNorm, eps=1e-6), **kwargs)\n",
    "#     return model\n",
    "\n",
    "# def vit_large_patch16(**kwargs):\n",
    "#     \"\"\"ViT-Large as defined in: An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale\"\"\"\n",
    "#     model = timm.models.vision_transformer.VisionTransformer(\n",
    "#         patch_size=16, embed_dim=1024, depth=24, num_heads=16, mlp_ratio=4, qkv_bias=True,\n",
    "#         norm_layer=partial(nn.LayerNorm, eps=1e-6), **kwargs)\n",
    "#     return model\n",
    "\n",
    "# def vit_huge_patch14(**kwargs):\n",
    "#     \"\"\"ViT-Huge as defined in: An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale\"\"\"\n",
    "#     model = timm.models.vision_transformer.VisionTransformer(\n",
    "#         patch_size=14, embed_dim=1280, depth=32, num_heads=16, mlp_ratio=4, qkv_bias=True,\n",
    "#         norm_layer=partial(nn.LayerNorm, eps=1e-6), **kwargs)\n",
    "#     return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "719b58ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose the weights and the architecture\n",
    "chkpts_finetuned = {\n",
    "    \"base\": \"mae_finetuned_vit_base.pth\",\n",
    "    \"large\": \"mae_finetuned_vit_large.pth\",\n",
    "    \"huge\": \"mae_finetuned_vit_huge.pth\",\n",
    "}\n",
    "chkpts_pretrained = {\n",
    "    \"base\": \"mae_pretrain_vit_base.pth\",\n",
    "    \"large\": \"mae_pretrain_vit_large.pth\",\n",
    "    \"huge\": \"mae_pretrain_vit_huge.pth\",\n",
    "}\n",
    "chkpts = {'pretrained': chkpts_pretrained, 'finetuned': chkpts_finetuned}[WEIGHTS_VERSION]\n",
    "\n",
    "model_architectures= {\n",
    "    \"base\": BASE_VIT,\n",
    "    \"large\": LARGE_VIT,\n",
    "    \"huge\": HUGE_VIT,\n",
    "}\n",
    "\n",
    "model_arch = model_architectures[MODEL_SIZE]\n",
    "chkpt_dir = os.path.join(WEIGHTS_FOLDER, chkpts[MODEL_SIZE])\n",
    "print(f\"Weights directory: \\n\\t{chkpt_dir}\\nModel architecture: \\n\\t{model_arch}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37e8b25c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the classification head of the ViT. Should be a torch.nn.module\n",
    "\n",
    "class PassThrough(nn.Module):\n",
    "    \"\"\"\n",
    "        This is a dummy head that just passes through the encoder output\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x\n",
    "    \n",
    "# ViT_head = PassThrough()\n",
    "\n",
    "\"\"\" Use a linear layer as classification head. \"\"\"\n",
    "ViT_head = nn.Linear(model_arch['embed_dim'], NO_CLASSES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a72a522",
   "metadata": {},
   "outputs": [],
   "source": [
    "# instantiate the model\n",
    "vision_transformer = prepare_vision_transformer(\n",
    "    checkpoint_directory=chkpt_dir,\n",
    "    model_architecture=model_arch,\n",
    "    classification_head=ViT_head,\n",
    ")\n",
    "# Output should be: <All keys matched successfully>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "052b12cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "summary(vision_transformer, (3, 224, 224), device='cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86bfd792",
   "metadata": {},
   "source": [
    "# Load the data\n",
    "https://pytorch.org/tutorials/beginner/data_loading_tutorial.html\n",
    "\n",
    "A class that contains the data. Extra data augmentation can be easily added. I already implemented the resize since the input images do not have the same size, which causes error when making a torch.Tensor with a batch of images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1997b8c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf4e1b7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DiabeticRetinopathyDataset(Dataset):\n",
    "    \"\"\"The Diabetic Retinopathy dataset from Kaggle.\"\"\"\n",
    "\n",
    "    def __init__(self, csv_file: str, root_dir: str, image_dir: str, size: int = None, transform=None):\n",
    "        \"\"\"\n",
    "        Arguments:\n",
    "            csv_file (string): The csv file with the labels\n",
    "            root_dir (string): The directory where the data is stored. The csv file should be in this directory.\n",
    "            image_dir (string): The path to the directory with the images from the root_dir.\n",
    "            size (int): Only consider the first number of samples from the csv file. \n",
    "            transform (Callable, optional): Optional transform to be applied on samples\n",
    "        \"\"\"\n",
    "        self.df = pd.read_csv(os.path.join(root_dir, csv_file))\n",
    "        self.root_dir = root_dir\n",
    "        self.image_dir = os.path.join(root_dir, image_dir)\n",
    "        self.transform = transform\n",
    "        self.ToTensor = transforms.ToTensor()\n",
    "        \n",
    "        self.items = self.df.iloc[:, 0]\n",
    "        self.labels = self.df.iloc[:, 1]\n",
    "        if size is not None:\n",
    "            self.items = self.items[:size]\n",
    "            self.labels = self.labels[:size]\n",
    "\n",
    "    def __getitem__(self, idx) -> torch.Tensor:\n",
    "        '''\n",
    "        Can obtain images as tensors from dataset by giving an index or \n",
    "        list/np.array/torch.Tensor of indices as input.\n",
    "        \n",
    "        Return (torch.Tensor, torch.Tensor): a batch of images (B x C x H x W), labels (B)\n",
    "        \n",
    "        '''\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "        if isinstance(idx, int):\n",
    "            idx = [idx]\n",
    "        \n",
    "        assert max(idx) < len(self.items), \"Outside the size of the dataset\"\n",
    "        \n",
    "        selected_items = self.items[idx]\n",
    "        sel_imgs_paths = [os.path.join(self.image_dir, im) + '.jpeg' for im in self.items[idx].tolist()]\n",
    "        sel_labels = self.labels[idx].tolist()\n",
    "        images = [Image.open(p) for p in sel_imgs_paths]\n",
    "        \n",
    "        if self.transform:\n",
    "            images = self.transform(images)\n",
    "            \n",
    "        images_tensors = [self.ToTensor(im) for im in images]\n",
    "        \n",
    "        return torch.stack(images_tensors, dim=0).squeeze(), torch.Tensor(sel_labels).type(torch.int64).squeeze()\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.items)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "090113af",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Resize(object):\n",
    "    \"\"\"\n",
    "    A class to resize the samples used for data augmentation.\n",
    "    \n",
    "    input is list of PIL objects, output should be list of PIL objects\n",
    "    \n",
    "    Arguments:\n",
    "        output_size (type): ...\n",
    "    \"\"\"\n",
    "    def __init__(self, output_size: int):\n",
    "        self.output_size = output_size\n",
    "        \n",
    "    def __call__(self, samples):\n",
    "        samples = [im.resize((self.output_size,)*2) for im in samples]\n",
    "        return samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9de832cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "DR_dataset = DiabeticRetinopathyDataset(\n",
    "    TRAIN_LABELS_CSV,\n",
    "    DATA_FOLDER,\n",
    "    TRAIN_FOLDER,\n",
    "    transform=transforms.Compose([Resize(output_size=224)]),  # output size depends on the model\n",
    "    size=800,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61b73b3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize some data\n",
    "def visualise_batch(images, labels):\n",
    "    for i, im in enumerate(images):\n",
    "        ax = plt.subplot(1, len(labels), i+1)\n",
    "        ax.set_title(f\"{labels[i].tolist()}\")\n",
    "        ax.imshow(im.permute(1, 2, 0))\n",
    "    \n",
    "visualise_batch(*DR_dataset[[1, 2, 3]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f080236a",
   "metadata": {},
   "source": [
    "# Training loop\n",
    "https://pytorch.org/tutorials/beginner/introyt/trainingyt.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a3256d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db6bd957",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_loader = torch.utils.data.DataLoader(DR_dataset, batch_size=BATCH_SIZE, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3ad206e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch(epoch_index):\n",
    "    running_loss = 0.\n",
    "    avg_loss = 0.\n",
    "\n",
    "    for i, data in tqdm(enumerate(training_loader)):\n",
    "        # Every data instance is an input + label pair\n",
    "        inputs, labels = data\n",
    "        inputs = inputs.to(DEVICE)\n",
    "        labels = labels.to(DEVICE)\n",
    "        \n",
    "        # Zero your gradients for every batch!\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Make predictions for this batch\n",
    "        outputs = vision_transformer(inputs)\n",
    "        # Nonsens to make the shape align\n",
    "        outputs = outputs[:, :5]\n",
    "\n",
    "        # Compute the loss and its gradients\n",
    "#         print(f\"Model output: \\n{outputs}\")\n",
    "#         print(f\"labels: \\n{labels}\")\n",
    "        loss = loss_fn(outputs, labels)\n",
    "        loss.backward()\n",
    "\n",
    "        # Adjust learning weights\n",
    "        optimizer.step()\n",
    "\n",
    "        # Gather data and report\n",
    "        running_loss += loss.item()\n",
    "        \n",
    "        # Print an update every 20 batches\n",
    "        if i % 20 == 19:\n",
    "            avg_loss = running_loss / 100 # loss per batch\n",
    "            print(f\"  batch {i+1} loss: {avg_loss}\")\n",
    "            running_loss = 0.\n",
    "\n",
    "    return avg_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e773f598",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.SGD(vision_transformer.parameters(), lr=0.001, momentum=0.9)\n",
    "loss_fn = torch.nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a4f6d1b",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Freeze the whole model, except the classification head\n",
    "for param in vision_transformer.parameters():\n",
    "    param.requires_grad = False\n",
    "for param in vision_transformer.head.parameters():\n",
    "    param.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "711ed0af",
   "metadata": {},
   "outputs": [],
   "source": [
    "vision_transformer.to(DEVICE)\n",
    "for epoch in range(EPOCHS):\n",
    "    vision_transformer.train(True)\n",
    "    avg_loss = train_one_epoch(epoch+1)\n",
    "    \n",
    "    # Set the model to validation mode\n",
    "    vision_transformer.eval()\n",
    "    # Run on the validation set and calculate performance\n",
    "    # TODO"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
